---
layout: post
title: RNN的前向传播
tags: deep-learning
---
循环神经网络（RNN）与普通的神经网络相比，循环神经网络能够记住上一次的信息，就是在隐藏层输入的时候，不仅仅接受来自与x的输入，还接受来自于上一次隐藏层的值。

##### 常见的RNN示意图：
![iamge](http://read.html5.qq.com/image?src=forum&q=5&r=0&imgflag=7&imageUrl=https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgYWFO7QJG5RblBnM5ibKVHp3tiaZ9vCdA7p6t27GW2CzMvQcRo5AtXX980DDibicD46vYhLspwvE0bpGQ/0?wx_fmt=png)

其中的`U`是输入与隐藏层之间的权重矩阵，`W`是隐藏层以隐藏层之间的权重矩阵，`V`是隐藏层与输出层之间的权重矩阵。

##### 一个标准的RNN模型如图所示：
![image](http://read.html5.qq.com/image?src=forum&q=5&r=0&imgflag=7&imageUrl=https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgYWFO7QJG5RblBnM5ibKVHp3lBbEpxYWyK60xJlKsMp04GZm99Tibawo7Ouia7NCIYFBtJofupD64jcg/0?wx_fmt=png)

##### 主要计算入下：
![iamge](http://read.html5.qq.com/image?src=forum&q=5&r=0&imgflag=7&imageUrl=https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgYWFO7QJG5RblBnM5ibKVHp37GBsRS4RcKygOibPVtyQVMzxWIIphZick0hafmeiaYVcvqpKwUicFG2Txg/0?wx_fmt=png)

为了更好的理解执行过程，我画了一个RNN前向传播的流程执行图。首先输入经过嵌入层embed，然后再经过隐藏层，隐藏层输入当前的状态值s_t,然后再经过softmax输出。
![image](http://read.html5.qq.com/image?src=forum&q=5&r=0&imgflag=7&imageUrl=https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgYWFO7QJG5RblBnM5ibKVHp3bmticxjuHhJ9ChLPdIibHq0eU7PA1rs057QcgFtOcDLNgRuyWLRx4pVA/0?wx_fmt=png)
![iamge](http://read.html5.qq.com/image?src=forum&q=5&r=0&imgflag=7&imageUrl=https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgYWFO7QJG5RblBnM5ibKVHp3Pf9VrNBCFxIlvGKdT7N8YibhVz4RKMibe8aURo58Sdueqvu0vFiaaZXeA/0?wx_fmt=png)
![image](http://read.html5.qq.com/image?src=forum&q=5&r=0&imgflag=7&imageUrl=https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgYWFO7QJG5RblBnM5ibKVHp3XuWuK8ZMvfnV09NHSicVdOJic8nIo6hfiaEW0niaO007z6HCbelRoxK53Q/0?wx_fmt=png)

##### RNN前向传播的代码：
```py

def forward_propagation(self, x):
    # The total number of time steps
    T = len(x) #输入的总长度
    # During forward propagation we save all hidden states in s because need them later.
    # We add one additional element for the initial hidden, which we set to 0
    s = np.zeros((T + 1, self.hidden_dim))
    s[-1] = np.zeros(self.hidden_dim)#初始化隐藏层的值，一般为0
    # The outputs at each time step. Again, we save them for later.
    o = np.zeros((T, self.word_dim))
    # For each time step...
    for t in np.arange(T):
        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.
        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))#每次隐藏层的输入包括当前x的输入和上一次隐藏层的值
        o[t] = softmax(self.V.dot(s[t]))#隐藏层经过全连接和softmax层后输出
    return [o, s]
```

更好的博文请参考：

[Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)

[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

上面的文章写的真的很棒哦~