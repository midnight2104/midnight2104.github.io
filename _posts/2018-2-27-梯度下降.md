---
layout: post
title: 梯度下降
tags: deep-learning
---
- 权重：一个较大的权重意味着神经网络认为这个输入比其它输入更重要，较小的权重意味着数据不是那么重要。
- 偏置：更多可能性返回某一个结果（或者说更倾向于某种结果）
- 激活函数：将求和的结果转换成某种输出信号
- 误差 (error)：一个普遍的指标是误差平方和 sum of the squared errors (SSE)

SSE的公式如下：
![image](https://thumbnail0.baidupcs.com/thumbnail/88af2f4821dbf3f03ddd341a6c5da6c3?fid=357015718-250528-700291474121736&time=1519714800&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-iLEy6MYeyfQnTxu3hI2g40Wpnmo%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=1337046236200241632&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)
SSE的公式中，使用平方而不使用绝对值的原因是：平方后误差大的得到的惩罚也越大，平方后误差小的得到的惩罚也越小；平方也可以简化后续的计算。1/2 也是为了简化后续步骤计算。

**下图是简化的图形：**

![image](https://thumbnail0.baidupcs.com/thumbnail/bcbc263dc73886c56f62366a68e0d0b4?fid=357015718-250528-293918757893023&time=1519714800&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-hdePkPaRtaZ6Ew6Bhk9GlMsvTP0%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=1337177998813790967&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)

（我们的目的就是得到最小值时的权重值）

从图中我们知道：
  - 误差只与权重有关，通过一步步改变权重，可以得到误差的最小值。
  - 权重的更新与梯度的方向恰好相反。
  - 学习速率用来控制梯度下降中更新步长的大小。
  
![image](https://thumbnail0.baidupcs.com/thumbnail/d1d1df9312d1ce117fcdb057c5057d37?fid=357015718-250528-815096056489808&time=1519714800&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-OTn7rzfvlluotJjdFywAXqHLGGQ%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=1337040451409669796&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)


**梯度下降的代码实现**
```py
#现在假设只有一个输出单元，来把这个写成代码。我们还是用 sigmoid 来作为激活函数 f(h)。
# Defining the sigmoid function for activations 
# 定义 sigmoid 激活函数
def sigmoid(x):
    return 1/(1+np.exp(-x))

# Derivative of the sigmoid function
# 激活函数的导数
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Input data
# 输入数据
x = np.array([0.1, 0.3])
# Target
# 目标
y = 0.2
# Input to output weights
# 输入到输出的权重
weights = np.array([-0.8, 0.5])

# The learning rate, eta in the weight step equation
# 权重更新的学习率
learnrate = 0.5

# the linear combination performed by the node (h in f(h) and f'(h))
# 输入和权重的线性组合
h = x[0]*weights[0] + x[1]*weights[1]
# or h = np.dot(x, weights)

# The neural network output (y-hat)
# 神经网络输出
nn_output = sigmoid(h)

# output error (y - y-hat)
# 输出误差
error = y - nn_output

# output gradient (f'(h))
# 输出梯度
output_grad = sigmoid_prime(h)

# error term (lowercase delta)
error_term = error * output_grad

# Gradient descent step 
# 梯度下降一步
del_w = [ learnrate * error_term * x[0],
          learnrate * error_term * x[1]]
# or del_w = learnrate * error_term * x
```
