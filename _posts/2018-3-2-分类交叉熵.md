---
layout: post
title: 分类交叉熵...
tags: deep-learning
---
分类交叉熵
之前，我们使用平方误差的和作为网络的成本函数，但是当时我们只有单个（标量）输出值。

但是当你在使用 softmax 时，输出是向量。一个向量是输出单元的概率值。你还可以使用一种叫独热编码(one-hot encoding)的方法，用向量表示数据标签。

这只是表示你有一个长度为类别数量的向量，标签元素标记为 1，而其他标签设为 0。对于之前的数字分类示例，图片数字 4 的标签向量是：

y=[0,0,0,0,1,0,0,0,0,0]

输出预测向量为：

y= [ 0.047, 0.048, 0.061, 0.07, 0.330,0.062, 0.001, 0.213, 0.013, 0.150] 


我们希望误差与这些向量之间的距离成比例。要计算这一距离，我们将使用交叉熵。我们的神经网络训练目标将为：==通过尽可能地减小交叉熵使预测向量与标签向量尽量靠近==。交叉熵计算公式如下所示：
![image](https://thumbnail0.baidupcs.com/thumbnail/1435a750b69ccd7425173bf8d426cbdb?fid=357015718-250528-47677925393277&time=1519956000&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-ZxXx%2BHi0CSFMyl05%2FAXDt9sY2JQ%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=1401942280459357424&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)

可以从上文中看出，交叉熵等于标签元素的和乘以预测概率的自然对数。注意，该等式并不对称！千万不能交换向量，因为标签向量里有很多 0，对 0 求对数将产生错误。

对标签向量使用独热编码的好处是除了为真的标签数值是1之外，其他所有的 y_j项都为 0 。因此，除了y_j = 1，其他所有项加起来为0，交叉熵直接变成
![image](https://thumbnail0.baidupcs.com/thumbnail/8c2e48833e1c83a3ed65f500f192f8d8?fid=357015718-250528-730271216519930&time=1519956000&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-rnvZ2E7SGqOUaloQiJ4rzrs29sk%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=1401995580899004583&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)
例如，输入图片为数字 4 并且标为 4，那么只有与 4 对应的单元的输出，在交叉熵成本函数中才会产生影响。
