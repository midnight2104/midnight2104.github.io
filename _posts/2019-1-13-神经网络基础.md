---
layout: post
title: 神经网络基础
tags: deep-learning
---

##### 1.为什么需要激活函数？
没有激活函数相当于矩阵相乘，多层和一层一样，只能拟合线性函数


##### 2.万有逼近定理
如果一个隐层包含足够多的神经元，三层前馈神经网络（输入-隐层
-输出）能以任意精度逼近任意预定的连续函数。

##### 3.双隐层感知器逼近非连续函数
当隐层足够宽时，双隐层感知器（输入-隐层1-隐层2-输出）可以逼
近任意非连续函数：可以解决任何复杂的分类问题。


##### 4.神经网络每一层的作用
- 每一层的数学公式：`y=a(wx+b)`，完成输入到输出的空间变换
- `wx`:升维/降维、放大/缩小、旋转
- `b`:平移
- `a(.)`:弯曲

> 神经网络学习如何利用矩阵的线性变换加激活函数的非线性变换，
将原始输入空间投影到线性可分的空间去分类/回归。
> 增加节点数：增加维度，即增加线性转换能力。
> 增加层数：增加激活函数的次数，即增加非线性转换次数

##### 5.网络更深or更宽？
在神经元总数相当的情况下，增加网络深度可以比增加宽度带来
更强的网络表示能力：产生更多的线性区域

##### 6.机器学习中的数学基础
- 线性代数：数据表示、空间变换的基础
- 概率论：模型假设、策略设计的基础
- 最优化：求解目标函数的具体算法

##### 7.矩阵特征向量
矩阵变换时，只有尺度变换而没有方向变换的向量就是它的特征向量

##### 8.矩阵的秩
 **线性方程组的角度**：度量矩阵行列之间的相关性
- 如果矩阵的各行或列是线性无关的，矩阵就是满秩的，也就是秩等于行数

 **数据点分布的角度**：表示数据需要的最小的基的数量
- 数据分布模式越容易被捕捉，即需要的基越少，秩就越小
-  数据冗余度越大，需要的基就越少，秩越小
-  若矩阵表达的是结构化信息，如图像、用户-物品表等，各行之间存在一定相关性，一般是低秩的

##### 9.欠拟合 vs 过拟合
- 欠拟合：训练集的一般性质尚未被学习器学好. (在训练集上误差都还很大)
> 欠拟合解决办法：增加网络层数、每层节点数、训练周期
- 过拟合：学习器把训练集特点当做样本的一般特点. (在训练集上误差很小了，但是在测试集上误差还是很大)
> 过拟合解决办法：dropout、early stop、regularization 

##### 10.频率学派 vs 贝叶斯学派
- 频率学派：参数估计只依赖观测数据；设计不同概率模型去拟合数据背后的规律，用拟合出的规律去推断和预测未知的结果
- 贝叶斯学派：参数估计同时依赖观测数据和先验知识；根据数据+变量的先验分布，同时推断每个变量以及变量之间关系的后验分布
