---
layout: post
title: 卷积神经网络
tags: deep-learning
---
**卷积神经网络**：在空间上共享参数的网络，用共享权重的卷积层替换了一般的全连接层。（假设在一张图片中要识别出其中的猫，不管猫在图片的任何位置，都可以使用相同的权重去寻找猫。当我们试图识别一个猫的图片的时候，我们并不在意猫出现在哪个位置。无论是左上角，右下角，它在你眼里都是一只猫。我们希望 CNNs 能够无差别的识别，这如何做到呢？如我们之前所见，一个给定的 patch 的分类，是由 patch 对应的权重和偏置项决定的。
如果我们想让左上角的猫与右下角的猫以同样的方式被识别，他们的权重和偏置项需要一样，这样他们才能以同一种方法识别。
这正是我们在 CNNs 中做的。）

 CNN 的学习方式：它学习识别基本的直线，曲线，然后是形状，点块，然后是图片中更复杂的物体。最终 CNN 分类器把这些大的，复杂的物体综合起来识别图片。CNN 是自主学习。

滤波器滑动的间隔被称作 stride（步长）。这是你可以调节的一个超参数。增大 stride 值后，会减少每层总 patch 数量，因此也减小了模型大小。通常这也会降低图像精度。
卷积层滤波器的数量被称为滤波器深度

![image](https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgbKsuEUmZ1mxPDWmNsy00NKliarEGmkhB9LIT19icJuRyI1kx56P5ykp4HwmkCv3xatyibV27h5BZ0mg/0?wx_fmt=png)

**维度**
综合目前所学的知识，我们应该如何计算 CNN 中每一层神经元的数量呢？
输入层（input layer）维度值为W， 滤波器（filter）的维度值为 F (height * width * depth)， stride 的数值为 S， padding 的数值为 P， 下一层的维度值可用如下公式表示: (W−F+2P)/S+1。 滤波器的数量就是下一层的深度（相当于每个滤波器扫描一次得到一层）。

TensorFlow 使用如下等式计算 SAME 、PADDING
SAME Padding， 输出的高和宽，计算如下：
```py
out_height = ceil(float(in_height) / float(strides[1]))
out_width = ceil(float(in_width) / float(strides[2]))
```
VALID Padding， 输出的高和宽，计算如下：
```py
out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))
out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))
```
**最大池化**
![image](https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgbKsuEUmZ1mxPDWmNsy00NKtbianzHRfX7hDwia5wohWauTQLO8ibs1MjEXiaibu4G1AaVJRoooYiaYkqBw/0?wx_fmt=png)
例如 [[1, 0], [4, 6]] 生成 6，因为 6 是这4个数字中最大的。同理 [[2, 3], [6, 8]] 生成 8。 理论上，最大池化操作的好处是减小输入大小（这样也会避免过拟合），使得神经网络能够专注于最重要的元素。最大池化只取覆盖区域中的最大值，其它的值都丢弃。
近期，池化层并不是很受青睐。部分原因是：
  - 现在的数据集又大又复杂，我们更关心欠拟合问题。
  - Dropout 是一个更好的正则化方法。
  - 池化导致信息损失。想想最大池化的例子，n 个数字中我们只保留最大的，把余下的 n-1 完全舍弃了。
  
 **卷积过程可视化**
 ![image](https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgbKsuEUmZ1mxPDWmNsy00NKOdMwyI1APGkjdLShvHtHXd0jv8VahCU8WvwCymyqIFwgoRqwk0NHPA/0?wx_fmt=png)
 ![image](https://mmbiz.qpic.cn/mmbiz_png/rw1wCRwDbgbKsuEUmZ1mxPDWmNsy00NKfQOn52spCs7N8g59qajicibHXQqeRgqvYlGiaGszKGnWoRib62ibgBUAVoQ/0?wx_fmt=png)
