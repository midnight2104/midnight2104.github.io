---
layout: post
title: 词嵌入word2vec
tags: NLP
---

word2vec作为神经概率语言模型的输入，是为了通过神经网络学习某个语言模型而产生的中间结果。具体来说，“某个语言模型”指的是`CBOW`和`Skip-gram`。具体学习过程会用到两个降低复杂度的近似方法——`Hierarchical Softmax`或`Negative Sampling`。两个模型乘以两种方法，一共有四种实现。


下面是手写版本的推导
#### CBOW + Hierarchical Softmax

![image](http://upyun.midnight2104.com/blog/20181028/word2vec1.png)

#### Skip-gram + Hierarchical Softmax

![image](http://upyun.midnight2104.com/blog/20181028/word2vec2.png)

#### CBOW + Negative Sampling

![image](http://upyun.midnight2104.com/blog/20181028/word2vec3.png)


#### Skip-gram + Negative Sampling

![image](http://upyun.midnight2104.com/blog/20181028/word2vec4.png)

#### 参考文献：
- [word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)
- [word2vec 中的数学原理详解](https://blog.csdn.net/itplus/article/details/37969519)